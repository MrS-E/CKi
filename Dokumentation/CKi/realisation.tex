\section{Allgemein}
\label{sec:RealAllgemein}
Die Realisation des Projekts CKI zeichnet sich durch die Implementierung eines modularen neuronalen Netzwerks aus, das für Aufgaben wie die Erkennung handschriftlicher Ziffern konzipiert wurde. Zum Einsatz kamen dabei Standard-C++-Technologien sowie eine CMake-basierte Projektstruktur für das Build-Management. Die Kernstruktur besteht aus Klassen für Neuron, Layer, und Network, die die Basis des Netzwerks bilden, ergänzt durch eine Util-Klasse für Hilfsfunktionen, wie Aktivierungsfunktionen und Datenverarbeitung.
Zusätzlich gibt es im Ordner „Test“, welche Unit-Tests mit den entsprechenden Attrappen für die Datensätze. Die Unit-Tests sind mit Google-Tests implementiert.
\\
Die klare Trennung zwischen den Komponenten des Netzwerks und die Nutzung von CMake unterstreichen einen modernen Ansatz in der Softwareentwicklung, der Flexibilität und Erweiterbarkeit des Projekts fördert. 

\section{Abhängigkeiten}
\label{sec:RealAbhängigkeiten}
Das Projekt CKI kommt mit nur wenigen Abhängigkeiten aus.
Die dies es dennoch gibt, sind umso wichtiger für das erfolgreiche Ausführen der Applikation.
\\
Abhängigkeiten:
\begin{enumerate}
	\item \textbf{googletest:} \\
	\textbf{Version (Tag):} v1.14.0 \\
	\textbf{Git:} \url{https://github.com/google/googletest} \\
	\textbf{Beschreibung:} GoogleTest ist ein C++-Framework für Unit-Tests, das Assertions für die Überprüfung von Code und Funktionen für die Organisation und Ausführung von Tests bietet.

	\item \textbf{nlohmann/json:} \\
	\textbf{Version (Tag):} v3.11.3 \\
	\textbf{Git:} \url{https://github.com/nlohmann/json} \\
	\textbf{Beschreibung:} Nlohmann/json ist eine moderne, header-only C++ Bibliothek für die Verarbeitung von JSON-Daten, die einfache Integration und intuitive Nutzung bietet.

	\item \textbf{wichtounet/mnist:} (nicht mehr in Verwendung)\\
	\textbf{Version (Commit):} 3b65c35 \\
	\textbf{Git:} \url{https://github.com/wichtounet/mnist} \\
	\textbf{Beschreibung:} Wichtounet/mnist ist ein einfacher C++-Reader für den MNIST-Datensatz, der es ermöglicht, Trainings- und Testbilder sowie Labels zu lesen und zu verwenden.

	\item \textbf{nothings/stb:}\\
	\textbf{Version (Commit):} 5736b15 \\
	\textbf{Git:} \url{https://github.com/nothings/stb} \\
	\textbf{Beschreibung:} Nothings/stb ist ein einfache Header-Library, um die Interkation mit Bildern zu vereinfachen.
\end{enumerate}

\section{Architektur}
\label{sec:RealArchitektur}
\subsection{Klassen}
\label{sec:RealKlassen}
\subsubsection{Klassendiagramm}
\label{sec:RealKlassendiagramm}
\begin{figure}[H]
	\centering
		\includegraphics[width=1.00\textwidth]{klassendiagramm_after_implementation.png}
		\caption{Klassendiagramm nach der Realisation}
	\label{fig:klassendiagramm after implementation}
\end{figure}
Das ist das Klassendiagramm, wie die Applikation des Projektes CKI nach der Implementation und somit nach den nötigen Anpassungen, die seit der Design-Phase zu treffen waren. Wie zu sehen ist, gibt es etliche markante Änderungen in den Klassen Netzwerk und Layer. Dies kommt daher, dass durch Wissensgewinn die Struktur angepasst werden musste. Wie diese Klassen funktionieren, sowie die wichtigsten Funktionen sind in den Abschnitten \ref{sec:RealKlassen}, \ref{sec:RealOrdnerstruktur}, \ref{sec:RealCode}, \ref{sec:RealSnippetsCode} zu finden.

\subsubsection{Netzwerk}
\label{sec:RealNetzwerk}
Die Network-Klasse repräsentiert das neuronale Netzwerk. Es unterstützt die Initialisierung des Netzwerks mit einer bestimmten Anzahl von Eingabe- und Ausgabeneuronen sowie eine variable Anzahl von versteckten Schichten und deren Grössen. Die Klasse bietet Funktionen für das Training des Netzwerks mit gegebenen Eingaben und Zielwerten, die Überprüfung der Netzwerkleistung, Vorhersagen für neue Eingaben, die Durchführung von Vorwärts- und Rückwärtspropagation, sowie das Speichern und Laden des Netzwerks in bzw. aus einer Datei. 
\subsubsection{Layer}
\label{sec:RealLayer}
Die Layer-Klasse definiert einen Layer im neuronalen Netzwerk, bestehend aus mehreren Neuronen. Sie bietet Funktionen zum Berechnen der Ausgaben der Neuronen basierend auf Eingaben, Initialisieren und Setzen von Gewichten, Aktualisieren von Gewichten und Biases auf Basis von Fehlern und Lernrate, sowie zur Fehlerberechnung im Vergleich zu Zielwerten. Jedes Layer-Objekt enthält eine Liste von Neuron-Objekten, die die Neuronen in diesem Layer repräsentieren. 
\subsubsection{Neuron}
\label{sec:RealNeuron}
Die Neuron-Klasse stellt ein einzelnes Neuron dar, inklusive seiner Gewichte, Eingaben, Aktivierungsfunktion, Bias und Summe der gewichteten Eingaben. Sie bietet Funktionen zur Berechnung der Aktivierung basierend auf den Eingaben, der Ableitung der Aktivierungsfunktion, der Fehlerberechnung im Vergleich zu einem Zielwert, sowie zur Speicherung der Gewichte und des Biases. 
\subsubsection{Utility}
\label{sec:RealUtility}
Die Util-Klasse bietet statische Hilfsfunktionen für das neuronale Netzwerk, darunter die Sigmoid-Aktivierungsfunktion und ihre Ableitung sowie Funktionen zum Lesen von MNIST-Bilddaten und Labels aus Dateien. Diese Hilfsfunktionen sind essenziell für die Vorverarbeitung von Eingabedaten und die Implementierung der Lernmechanismen im Netzwerk. 
\subsection{Ordnerstruktur}
\label{sec:RealOrdnerstruktur}
Im Kern des Projekts stehen die Hauptdateien, welche die essenziellen Klassen wie Neuron, Layer, Network und Util enthalten. Diese sind grundlegend für die Funktionalität des neuronalen Netzwerks. Die CMakeLists.txt unterstützt das Build-Management, vereinfacht die Kompilierung und Konfiguration. Der .gitignore sorgt dafür, dass unnötige Dateien und Ordner nicht in die Versionskontrolle einfliessen. Ein speziell dafür vorgesehener Test-Ordner beinhaltet Tests zur Überprüfung der Funktionalität, was die Zuverlässigkeit des Systems sicherstellt. Zusätzlich runden Dummy-Files und ein eigener Ordner für die MNIST-Datasets das Projekt ab, indem sie die Datenhaltung für Training und Tests des Netzwerks erleichtern. 
\\
Das CKI-Projekt ist strukturiert in:
\begin{itemize}
	\item \textbf{Hauptdateien:} 
	Enthalten die Klassen Neuron, Layer, Network, und Util für die Kernlogik des neuronalen Netzwerks.
  \item \textbf{CMakeLists.txt:} 
	Für das Build-Management erleichtert das Kompilieren und die Konfiguration des Projekts.
  \item \textbf{.gitignore:} 
	Definiert Dateien und Ordner, die von Git-Versionierung ausgeschlossen sind, wie Build-Artefakte und IDE-spezifische Dateien.
  \item \textbf{Test-Ordner:} 
	Beinhaltet Testfälle zur Überprüfung der Funktionalität einzelner Komponenten und des Gesamtsystems.
	\item \textbf{Dummy-Files:} 
	Zusätzliche Dateien für Testzwecke.
	\item \textbf{MNIST-Datasets Ordner:} 
	Speichert die Datensätze für das Training und Testen des Netzwerks, insbesondere für die Erkennung handschriftlicher Ziffern.
\end{itemize}

\section{Code}
\label{sec:RealCode}
\subsection{Konzept}
 Das Projekt CKI nutzt grundlegende Konzepte des maschinellen Lernens wie Vorwärts- und Rückwärts-Propagierung, Aktivierungsfunktionen (hier Sigmoid) und die Anpassung von Gewichten während des Trainingsprozesses. Die modulare Struktur, unterteilt in Schichten, Neuronen und Hilfsfunktionen, sowie die Verwendung von modernem C++ und externen Bibliotheken für Tests und Datenhandling, zeichnen die Architektur der Applikation aus.
\\
Um mehr über die hier erwähnten Funktionen zu erfahren, ist der Abschnitt \ref{sec:RealAlgorithmen} zu konsultieren.

\subsection{Algorithmen}
\label{sec:RealAlgorithmen}
\subsubsection{ Forward-Propagation}
\label{sec:RealForwardPropagation}
Die Forward-Propagation ist ein grundlegender Prozess in neuronalen Netzwerken, der es ermöglicht, Vorhersagen auf Basis von Eingabedaten zu treffen. Dabei werden die Eingabedaten durch das Netzwerk von der Eingabeschicht über eine oder mehrere versteckte Schichten bis zur Ausgabeschicht vorwärts geleitet. Jede Schicht besteht aus Neuronen, die über Gewichte mit den Neuronen der vorherigen Schicht verbunden sind. Die Daten werden in jedem Neuron durch eine Summationsfunktion verarbeitet, die die gewichteten Eingaben aufsummiert und einen Bias-Wert hinzufügt. Das Ergebnis dieser Summation wird dann durch eine Aktivierungsfunktion geleitet, um die Ausgabe des Neurons zu bestimmen.
\\
Die Aktivierungsfunktion bestimmt, wie Neuronen ihre Eingaben in Ausgaben umwandeln und ist entscheidend für die Fähigkeit des Netzwerks, komplexe Muster in den Daten zu erkennen. Beliebte Aktivierungsfunktionen sind die Sigmoid-, Tanh- und ReLU-Funktion.
\\
Sobald die Eingabedaten durch das Netzwerk propagiert worden sind und die Ausgabeschicht erreicht haben, wird das Ergebnis mit dem tatsächlichen Wert verglichen, um den Fehler der Vorhersage zu bestimmen. Dieser Fehler wird dann in einem separaten Prozess, der als Backpropagation bekannt ist, verwendet, um die Gewichte im Netzwerk anzupassen und die Vorhersagegenauigkeit zu verbessern.\footnote{vgl. Nüesch, 2023}
\subsubsection{Back-Propagation}
\label{sec:RealBackPropagation}
Die Backpropagation, kurz für „backward propagation of errors“, ist ein Schlüsselmechanismus im Training neuronaler Netzwerke. Dieser Algorithmus ermöglicht es, die Gewichte des Netzwerks so anzupassen, dass der Gesamtfehler bei der Vorhersage minimiert wird. Backpropagation wird nach der Forward-Propagation angewendet, nachdem eine Vorhersage durch das Netzwerk gemacht und der Fehler zwischen der Vorhersage und dem tatsächlichen Wert berechnet wurde.
\\
Der Prozess der Backpropagation besteht aus zwei Hauptphasen: der Berechnung des Gradienten des Fehlers bezüglich aller Gewichte im Netzwerk und der anschliessenden Anpassung dieser Gewichte in die Richtung, die den Fehler minimiert. Der Gradient gibt an, in welche Richtung die Gewichte verändert werden müssen, um den Fehler zu verringern, und die Grösse der Anpassung wird durch die Lernrate bestimmt.
\\
Backpropagation nutzt die Kettenregel der Differenzialrechnung, um die Fehlergradienten für die Gewichte jeder Schicht vom Ausgang zurück zum Eingang effizient zu berechnen. Der berechnete Fehlergradient für jede Gewichtung zeigt, wie eine kleine Änderung in diesem Gewicht den Gesamtfehler beeinflusst. Durch die systematische Anpassung der Gewichte basierend auf diesen Gradienten kann das Netzwerk schrittweise verbessert werden, um genauere Vorhersagen zu liefern.
\\
Insgesamt ermöglicht Backpropagation das effiziente Training tiefer neuronaler Netzwerke, indem es systematisch die Netzwerkgewichte anpasst, um den Fehler zwischen den Vorhersagen des Netzwerks und den tatsächlichen Werten zu minimieren, was zu einer verbesserten Modellleistung führt.\footnote{vgl. Nüesch, 2023}

\subsection{Schlüsselpassagen \& Snippets} %stolpersteine
\label{sec:RealSnippetsCode}
Die wichtigsten Funktionen in einem neuronalen Netzwerk sind die beiden Propagation, vorwärts als auch rückwärts. Wobei diese das Grundgerüst für die drei Hauptfunktionen, Train, Verify \& Predict bilden.

\subsubsection{Train}
\label{sec:RealTrainCode}
\begin{lstlisting}[language=C++]
double Network::train(std::vector<std::vector<double>> &inputs, std::vector<double> &labels, int epochs, double learning_rate){
    double total_error = 0;
    for(int epoch = 0; epoch<epochs; epoch++ ){
        for(std::size_t i = 0; i < inputs.size(); i++){
            std::vector<double> outputs = Network::forward_propagation(inputs[i]);
            total_error += Network::backward_propagation(labels[i], learning_rate);
            total_error /= 2;
        }
    }
    return total_error;
}

\end{lstlisting}
\addcontentsline{lol}{lstlisting}{\protect\numberline{\thelstlisting} Train Funktion aus der Netzwerk-Klasse}
Die Funktion, welche für das Lernen des neuronalen Netzwerks verantwortlich ist, ist simpel aufgebaut. Sie erhält primär zwei Listen: eine Liste aus Bildern und eine Liste aus Beschriftungen für die Bilder. Danach nutzt es die Funktionen der Forward- und Back-Propagation, um zuerst das Bild durch das Netzwerk bewerten zulassen und dann diese Bewertung mithilfe der Beschriftungen zu korrigieren. Dies tut sie für den gesamten Datensatz und der Anzahl Epochen entsprechend.

\subsubsection{Verify}
\label{sec:RealVerifyCode}
\begin{lstlisting}[language=C++]
double Network::verify(const std::vector<std::vector<double>> &inputs, const std::vector<double> &labels){
    int correct = 0;
    for (std::size_t i = 0; i < inputs.size(); ++i){
        if (Network::predict(std::vector<double>(inputs[i])) == static_cast<int>(labels[i])){
            correct++;
        }
    }
    return static_cast<double>(correct) / static_cast<double>(inputs.size());
}
\end{lstlisting}
\addcontentsline{lol}{lstlisting}{\protect\numberline{\thelstlisting} Verify Funktion aus der Netzwerk-Klasse}
Die Funktion ist eigentlich gleich aufgebaut wie „Train“, jedoch mit dem markanten Unterschied, dass keine Back-Propagation zum Einsatz kommt. So werden die Bewertungen des neuronalen Netzwerks nur als richtig oder falsch bewertet und nicht korrigiert.

\subsubsection{Predict}
\label{sec:RealPredictCode}
\begin{lstlisting}[language=C++]
int Network::predict(const std::vector<double> &input){
    std::vector<double> outputs = Network::forward_propagation(input);
    return static_cast<int>(std::distance(outputs.begin(), std::max_element(outputs.begin(), outputs.end())));
}
\end{lstlisting}
\addcontentsline{lol}{lstlisting}{\protect\numberline{\thelstlisting} Predict Funktion aus der Netzwerk-Klasse}
Diese Funktion erhält nur ein einziges Bild als Attribut. Dieses Bild wird durch die Forward-Propagation bewertet. Die Bewertung wiederum besteht nur aus einer Liste von zehn Prozentzahlen (für die Ziffern von 0 bis 9) und damit diese besser für einen Menschen lesbar wird, muss die höchste Prozentzahl aus der Liste gesucht werden. Deren Index wird dann als errechnete Zahl zurückgegeben.

\subsubsection{Forward-Propagation}
\label{sec:RealForwardPropagationCode}
\begin{lstlisting}[language=C++]
std::vector<double> Network::forward_propagation(const std::vector<double>& input){
    layers[0].calc_neuron_outputs(input);

    for (int i = 1; i < layers.size(); ++i){
        std::vector<double> inputs = layers[i - 1].get_neuron_outputs();
        layers[i].calc_neuron_outputs(inputs);
    }

    return layers[layers.size()-1].get_neuron_outputs();
}
\end{lstlisting}
\addcontentsline{lol}{lstlisting}{\protect\numberline{\thelstlisting} Forward-Propagation Funktion aus der Netzwerk-Klasse}
In dieser Funktion wird eine Liste von grossen Fliesskommazahlen übernommen. Diese sind die einzelnen Pixel, in dem zu bearbeiteten Bild, welche nur einen Helligkeitswert beinhalten. Diese Pixel werden an den ersten Layer im Netzwerk übergeben. Dieser kalkuliert durch die Sigmoid-Funktion neue Ausgabewerte. Die Ausgabewerte werden mit der For-Schleife durch alle Schichten des Netzwerks weitergereicht, bis diese am Ende ausgelesen und als Rückgabewert dieser Funktion genutzt werden. Die Ausgabewerte haben zwar das gleiche Datenformat, unterscheiden sich jedoch anhand ihrer Grösse (Länge der Liste) und den tatsächlichen Werten.

\subsubsection{Back-Propagation}
\label{sec:RealBackPropagationCode}
\begin{lstlisting}[language=C++]
double Network::backward_propagation(const int& target, double learning_rate){
    std::vector<double> inputs (10, 0);
    inputs[target] = 1;
    std::vector<double> error = layers[layers.size()-1].calculate_error(inputs);

    double total_error = 0;
    for(double& e : error) { //remove for better performance
        total_error += e;
    }

    total_error/=error.size();

    //std::cout << total_error << std::endl;

    for (int i = layers.size() - 1; i >= 0; --i){
        error = layers[i].update_weights_and_biases(error, learning_rate);
    }
    return total_error;
}
\end{lstlisting}
\addcontentsline{lol}{lstlisting}{\protect\numberline{\thelstlisting} Backward-Propagation Funktion aus der Netzwerk-Klasse}
Im Gegensatz zur Forward-Propagation wird in der Back-Propagation ein Wert nicht von der ersten Schicht zur letzten übergeben, sondern Rückwerts von dem letzten Layer bis zur Eingabe zurück.
\\ 
Dabei muss zuerst die Abweichung von der erhaltenen Ausgabe (aus der Forward-Propagation) mit der erwarteten Ausgabe abgeglichen und der Fehler berechnet werden. Nun wird ähnlich zur Forward-Propagation dieser Fehler durch die einzelnen Schichten geschickt und dort verwendet, um die Gewichtungen und Biases für jedes Neuron anzupassen. Dies passiert in einer eigenen Funktion im Layer.

\subsubsection{Anpassung von Gewichtungen und Biases}
\label{sec:RealAnpassungVonGewichtungenUndBiasesCode}
\begin{lstlisting}[language=C++]
std::vector<double> Layer::update_weights_and_biases(const std::vector<double>& error, double learning_rate){
    std::vector<double> prev_layer_error(Layer::neurons[0].weights.size(), 0.0);

    for (size_t i = 0; i < Layer::neurons.size(); ++i){
        Neuron &neuron = Layer::neurons[i];
        const double neuron_error = error[i];
        const double activation_derivative = neuron.activation_derivative();

        for (size_t j = 0; j < neuron.weights.size(); ++j){
            prev_layer_error[j] += neuron.weights[j] * neuron_error;

            const double delta_weight = neuron_error * activation_derivative * neuron.inputs[j];
            neuron.weights[j] -= learning_rate * delta_weight;
        }
        neuron.bias += learning_rate * neuron_error * activation_derivative;
    }
    return prev_layer_error;
}
\end{lstlisting}
\addcontentsline{lol}{lstlisting}{\protect\numberline{\thelstlisting} Anpassung von Gewichtungen und Biases aus der Layer-Klasse}
Dies ist die Funktion, die in der Back-Propagation aufgerufen wird.
\\
Dabei wird für jedes Neuron für jede Gewichtung eine eigene Anpassung gesucht. Um dies zu erreichen, werden mehrere Werte benötigt: der Fehler, der das Neuron geliefert hat, die Ableitung der Sigmoid-Funktion (die Aktivierungsfunktion in diesem Netzwerk). Jetzt können mehrere Sachen errechnet werden. 
\\
Der Fehler für den nächst oberen Layer, dieser setzt sich auseinander aus dem Fehler und dem entsprechenden Gewicht für das Neuron. Dabei wird der Fehler neu gewichtet und zusammen summiert mit allen anderen neu-gewichteten Fehlern für dieses eine Neuron auf der nächsten Ebene.
Die Abweichung der Gewichtung ist ein Produkt des Inputs in diesem Neuron, dem Fehler, der Ableitung der Sigmoid-Funktion und der Learningrate.
\\
Die Korrektion für das Bias ist fast identisch zur Abweichung der Gewichtung. Es ist ebenfalls ein Produkt der gleichen Faktoren, abgesehen von der Learningrate und dem Input.

\subsubsection{Sigmoid \& Ableitung}
\label{sec:RealSigmoidAbleitungCode}
\begin{lstlisting}[language=C++]
double Util::sigmoid(double x){
	return 1.0 / (1.0 + exp(-x));
}

double Util::sigmoid_derivative(double x){
	double s = sigmoid(x);
	return s * (1.0 - s);
}
\end{lstlisting}
\addcontentsline{lol}{lstlisting}{\protect\numberline{\thelstlisting} Sigmoid \& Ableitung aus der Util-Klasse}
Das sind die mathematischen Implementierungen der Sigmoid-Funktion und deren mathematischen Abweichungen. Diese Funktionen sind zuständig für die einzelnen Aktivierungen der Neuronen und verschieben eine Zahl in den Wertebereich zwischen 0 und 1.
\begin{figure}[H]
	\centering
		\includegraphics[width=0.75\linewidth]{sigmoid.png}
		\caption{Sigmoid Funktion}
	\label{fig:sigmoid}
\end{figure}

\subsubsection{MNIST-Reader}
\label{sec:RealMNISTReaderCode}
\begin{lstlisting}[language=C++]
std::vector<std::vector<double>> Util::read_mnist_images(const std::string &filename){
    std::ifstream file(filename, std::ios::binary);
    if (file.is_open()){
        int magic_number = read_int(file);
        int number_of_images = read_int(file);
        int number_of_rows = read_int(file);
        int number_of_columns = read_int(file);

        std::vector<std::vector<double>> images(number_of_images, std::vector<double>(number_of_rows * number_of_columns));

        for (int i = 0; i < number_of_images; ++i){
            for (int r = 0; r < number_of_rows * number_of_columns; ++r){
                unsigned char temp = 0;
                file.read(reinterpret_cast<char*>(&temp), sizeof(temp));
                images[i][r] = (double)temp / 255.0; // Normalizing pixel values to [0, 1]
            }
        }
        return images;
    }else{
        throw std::runtime_error("Cannot open file: " + filename);
    }
}
\end{lstlisting}
\addcontentsline{lol}{lstlisting}{\protect\numberline{\thelstlisting} MNIST-Reader aus der Util-Klasse}
Um die Daten aus den von \url{http://yann.lecun.com/exdb/mnist/} stammenden UByte-Dateien auszulesen, müssen gewisse Byte-Werte aus den Dateien ausgelesen werden. Die hier aufgeführte Funktion dient dazu, Bilder aus diesen Dateien auszulesen und ist beispielhaft auch für die Beschreibungen.
\\
Die Implementierung ist nach dem Muster von \url{http://yann.lecun.com/exdb/mnist/} implementiert. So sind in den ersten vier Byte gespeichert, wie viele Bilder/Beschriftungen im Datensatz zu finden sind. Um dies auszulesen, ist in einer eigenen Funktion implementiert (read\_int). Nach den ersten vier Byte müssen bestimmt viele Bytes in eine Liste eingetragen und auf den Wert zwischen 0 und 1 gebracht werden. Man spricht auch von der Normalisierung der Pixel-Werte.

\subsubsection{Ersten 4 Bytes}
\label{sec:RealErsten4BytesCode}
Dies ist die oben angesprochene „read\_int“-Funktion.
\begin{lstlisting}[language=C++]
int Util::read_int(std::ifstream &file){
    unsigned char bytes[4];
    file.read(reinterpret_cast<char*>(bytes), sizeof(bytes));
    return (int)((bytes[0] << 24) | (bytes[1] << 16) | (bytes[2] << 8) | bytes[3]);
}
\end{lstlisting}
\addcontentsline{lol}{lstlisting}{\protect\numberline{\thelstlisting} Utility Funktion für den MNIST-Reader aus der Util-Klasse}
Die Funktion wurde mit der Hilfe von Stack Overflow unter dem Link \url{https://stackoverflow.com/questions/604431/c-reading-unsigned-char-from-file-stream} erstellt.
\\
Zuerst wird ein Array in der Grösse von vier Bytes erstellt und vier Bytes aus einer Datei eingelesen. Danach werden die vier Bytes zu einer einzigen ganzen Zahl zusammengesetzt. Dies geschieht, indem die Bytes um jeweils 8 Bits nach Links verschoben werden und somit das erste Byte die acht höchsten Stellen der ganzen Zahl symbolisiert, danach das zweite Byte und so folgend die restlichen Bytes.
